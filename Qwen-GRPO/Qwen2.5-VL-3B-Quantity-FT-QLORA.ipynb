{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5128f6d9-141f-4c2b-b967-13f1a6b8514b",
   "metadata": {},
   "source": [
    "Ref:\n",
    "\n",
    "Finetuning Script: https://github.com/zhangfaen/finetune-Qwen2-VL/blob/main/finetune.py\n",
    "\n",
    "Repo: https://github.com/zhangfaen/finetune-Qwen2-VL\n",
    "\n",
    "#### Compatibility: Qwen2.5 requires Transformers 4.52 version (present in backup_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36b706f-34ba-4338-b841-6afcf60aa426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "print(os.getenv(\"CONDA_DEFAULT_ENV\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f05fb-43e4-4267-98c9-103be86d35f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c9b73-493d-422d-8ef5-b964afaa84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "import datasets\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e3336-6f77-4c7d-8cc3-8a0ba7cad759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset\n",
    "from functools import partial\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408bf84-2f39-4437-a2bb-a34ab994cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "from util.logutil import init_logger, get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7529beb3-9a46-4dd4-b409-970eff68fc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4e6948-e64a-481a-bae0-cc0421f063a4",
   "metadata": {},
   "source": [
    "### Initialize Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc747f10-abbe-4dea-b264-e2a29b49585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"train_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\n",
    "    \"[%(asctime)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "console = logging.StreamHandler()\n",
    "console.setFormatter(formatter)\n",
    "logger.addHandler(console)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24e647e-f157-4a14-9d4e-03476175ff34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "143c758b-a507-42f4-8037-490988abb846",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fb3123-2ff1-4aef-9287-417a25278703",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"dutta18/Quantity-Reasoning-VQA-23K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2545d46a-2d8a-48f8-ace2-4437a5746e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960b3941-eecd-418f-bf1e-62a26ff2cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcaf610-a654-4d4f-88ff-7decf13942ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.select(range(4450))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df5ad97-035a-4da3-9db0-046515fd1299",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65cd2a-2956-4c2f-a67f-63264b1a4225",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b2fe1-7349-451c-bcd7-49d26033229d",
   "metadata": {},
   "source": [
    "### Load COT Think Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e83ece5-9060-4d10-864b-388b12115a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./qty-reasoning-cot-data-8000.pkl', 'rb') as file:\n",
    "    cot_think_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafdb398-307c-4c26-9f29-542fb28e77ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_think_data = cot_think_data[:4450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7840b707-7ca9-4d6e-972b-cb459b2d5137",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cot_think_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af90f58d-3f56-477e-92ff-115ba255f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.add_column(\"cot_think_data\", cot_think_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98443293-43e3-4756-8ee8-57854190cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ccd8b2-7706-4aca-b805-55b306a63f8e",
   "metadata": {},
   "source": [
    "### Split Into Train, Test & Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a650074-ab94-4c04-aa4c-44ea9e6647d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c0a20-6522-4e1c-a830-47ee2bb07899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First create train (80%) and temp (20%)\n",
    "train_test = dataset.train_test_split(test_size=0.25, seed=42)\n",
    "\n",
    "# 2. Split the temp set into validation (10%) and test (10%)\n",
    "test_val = train_test['test'].train_test_split(test_size=0.6, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39766870-8479-4a33-b33c-a11b403f130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {\n",
    "    'train': train_test['train'],\n",
    "    'validation': test_val['train'],\n",
    "    'test': test_val['test'],\n",
    "}\n",
    "\n",
    "dataset_dict = DatasetDict(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf5ac6-8024-4a03-a5e0-2597acc6fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = dataset_dict['train'], dataset_dict['validation'], dataset_dict['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be071245-5244-4d56-87da-a1024812343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4178756e-ff3e-4c12-be1c-16b2a5ec14ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958d6169-b972-4a21-be5e-8c8f61d23882",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39d8328-d014-4e62-9f11-1441b3335334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eda11bc6-dc23-4d0c-afaf-198deb92a31a",
   "metadata": {},
   "source": [
    "### Creating JSON Format of the AOKVQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9038e3d-a1a3-4454-b212-99ca3cbd1e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "formattedJSONTrain = list()\n",
    "\n",
    "for idx in tqdm(range(len(train_set))):\n",
    "    currentJSON =   {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"image\": train_set[idx]['image']\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\", \n",
    "                            \"text\": train_set[idx]['question']\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\", \n",
    "                            \"text\": train_set[idx]['cot_think_data']\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }  \n",
    "    formattedJSONTrain.append(currentJSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070efa6-e6bf-4c47-99b7-0c3928f0f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "formattedJSONVal = list()\n",
    "\n",
    "for idx in tqdm(range(len(val_set))):\n",
    "    currentJSON =   {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"image\": val_set[idx]['image']\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\", \n",
    "                            \"text\": val_set[idx]['question']\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\", \n",
    "                            \"text\": val_set[idx]['cot_think_data']\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }  \n",
    "    formattedJSONVal.append(currentJSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22599481-caf1-4e2d-b85d-561b3ac693ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "formattedJSONTest = list()\n",
    "\n",
    "for idx in tqdm(range(len(test_set))):\n",
    "    currentJSON =   {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"image\": test_set[idx]['image']\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\", \n",
    "                            \"text\": test_set[idx]['question']\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\", \n",
    "                            \"text\": test_set[idx]['cot_think_data']\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }  \n",
    "    formattedJSONTest.append(currentJSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3431c7-3222-4789-982f-af3d5fdb3bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "782a994a-e9e0-47ec-9923-de33f12d93b2",
   "metadata": {},
   "source": [
    "### Prepare Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88da1581-779d-4c1f-bd35-b55376e16f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdb84cd-56ed-4cdb-9c2b-c83d4287b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class qtyDataset(Dataset):\n",
    "    def __init__(self, formatted_json_data):\n",
    "        super().__init__()\n",
    "        self.data = formatted_json_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f112c7a-c040-48bb-9ec4-997f3efb7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c346273f-9cd0-4aa3-aa5d-4b6932e8b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = qtyDataset(formattedJSONTrain)\n",
    "val_dataset = qtyDataset(formattedJSONVal)\n",
    "test_dataset = qtyDataset(formattedJSONTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b775d-b0b4-4b67-adfa-86c4832933e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a22b92f-5cee-4c71-930e-0a6c973b1bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_assistant_content_sublist_indexes(l):\n",
    "    '''\n",
    "    A message from train_data/data.json may look like below:\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {'role': 'user', 'content': [{'type': 'image', 'image': 'train_data/1.jpeg'}, {'type': 'text', 'text': '描述一下这个图片'}]}, \n",
    "                {'role': 'assistant', 'content': [{'type': 'text', 'text': '这张图片展示了一位年轻女子和她的狗在海滩上玩耍的场景。女子穿着格子衬衫和黑色裤子，坐在沙滩上，与她的金毛犬互动。她们的手臂伸展着，似乎在进行某种游戏或训练。背景是广阔的海洋和晴朗的天空，阳光洒在沙滩上，营造出温暖而宁静的氛围。整体画面充满了快乐和放松的感觉。'}]}\n",
    "            ]\n",
    "        }\n",
    "    After apply_chat_template, the text will look like below:\n",
    "        ['<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>描述一下这个图片<|im_end|>\\n<|im_start|>assistant\\n这张图片展示了一位年轻女子和她的狗在海滩上玩耍的场景。女子穿着格子衬衫和黑色裤子，坐在沙滩上，与她的金毛犬互动。她们的手臂伸展着，似乎在进行某种游戏或训练。背景是广阔的海洋和晴朗的天空，阳光洒在沙滩上，营造出温暖而宁静的氛围。整体画面充满了快乐和放松的感觉。<|im_end|>\\n']\n",
    "\n",
    "    This function tries to find the indexes of the assistant content in the input_ids list to build labels.\n",
    "    '''\n",
    "    start_indexes = []\n",
    "    end_indexes = []\n",
    "\n",
    "    # Iterate through the list to find starting points\n",
    "    for i in range(len(l) - 1):\n",
    "        # Check if the current and next elements form the start sequence\n",
    "        if l[i] == 151644 and l[i+1] == 77091 and l[i+2] == 198:\n",
    "            start_indexes.append(i+3)\n",
    "            # Now look for the first 151645 and 198 after the start\n",
    "            for j in range(i+3, len(l)-1):\n",
    "                if l[j] == 151645 and l[j+1] == 198:\n",
    "                    end_indexes.append(j+2) # **NOTE** the <|im_end|>\\n 2 tokens should be included in the label, so that model can predicate end of output.\n",
    "                    break  # Move to the next start after finding the end\n",
    "\n",
    "    return list(zip(start_indexes, end_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4274e9b9-9228-48f2-81cd-b6ead522b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, processor, device):\n",
    "    \n",
    "    messages = [m['messages'] for m in batch]\n",
    "    texts = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=False) for msg in messages]\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=texts,\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    input_ids_lists = inputs['input_ids'].tolist()\n",
    "    assert len(messages) == len(input_ids_lists)\n",
    "\n",
    "    labels_list = []\n",
    "    for ids_list in input_ids_lists:\n",
    "        label_ids = [-100] * len(ids_list)\n",
    "        for begin_end_indexs in find_assistant_content_sublist_indexes(ids_list):\n",
    "            label_ids[begin_end_indexs[0]:begin_end_indexs[1]] = ids_list[begin_end_indexs[0]:begin_end_indexs[1]]\n",
    "        labels_list.append(label_ids)\n",
    "\n",
    "    labels_ids = torch.tensor(labels_list, dtype=torch.int64)\n",
    "    return inputs, labels_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58a7adf-b82d-4525-9f90-bac43177e443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d76bce9-58e3-4045-9096-247b713eedcd",
   "metadata": {},
   "source": [
    "### Model Loading & Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723aa1be-7ffa-4463-973b-3da0884f958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59b6e1c-d112-41d4-a896-7bbd9d99f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization Configuration.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Use float16 for computation\n",
    "    bnb_4bit_use_double_quant=True,  \n",
    "    bnb_4bit_quant_type=\"nf4\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c03787-c6eb-44fd-8b05-d847788e81f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage = True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# Load processor. \n",
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", \n",
    "    min_pixels=256*28*28, \n",
    "    max_pixels=512*28*28, \n",
    "    padding_side=\"left\",\n",
    "    use_fast=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b61b18-c63f-49e3-8424-b66e6465f2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/home/aritrad/MOE-Directory/temp/Llama-3.2-11B-Vision-Instruct\")\n",
    "processor.save_pretrained(\"/home/aritrad/MOE-Directory/temp/Llama-3.2-11B-Vision-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1141ff7f-ddfc-4a5d-bc42-6dd1c033fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc725db-190a-4cea-be59-086b1c3ba60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2448d86-77d3-4f57-a788-d65fabbe1986",
   "metadata": {},
   "source": [
    "### LORA Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1caf42-7579-4eb6-8d5d-5cd336ee42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1c819d-ed75-441b-940d-ae98556e713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LORA_Rank = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57898f7b-ae0c-4d29-b9a5-5d1dd6cdfc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=LORA_Rank,\n",
    "    lora_alpha=LORA_Rank*2,  \n",
    "    lora_dropout=0.05,  \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"attn.qkv\", \"attn.proj\"], \n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197f9484-ed6b-4e87-86ba-ac193c3e4134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "181e9a83-59f6-494c-a016-264da818bfdb",
   "metadata": {},
   "source": [
    "#### Get PEFT Wrapper Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50543354-e20c-4b33-b833-477917d0daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qlora_qwen_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51288b20-7a5e-41de-a91f-da07aa4221fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_trainable_params():\n",
    "    \n",
    "    # Simple param report\n",
    "    trainable = sum(p.numel() for p in qlora_qwen_model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable params: {trainable/1e6:.1f} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527d3f2a-10c7-4f4f-8eb0-1f1700704b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_trainable_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e23eae2-9e85-45ec-9297-2305ae39b132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e3d87d8-29b8-43c3-8c52-aa17d6dccc75",
   "metadata": {},
   "source": [
    "### Create & Test Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467f05ab-04fb-49d3-94f8-f7a6e9cfb797",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize_ = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1408b95-4f05-4568-8db1-fd50990098fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batchSize_,\n",
    "    collate_fn=partial(collate_fn, processor=processor, device=device),\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batchSize_,\n",
    "    collate_fn=partial(collate_fn, processor=processor, device=device)\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batchSize_,\n",
    "    collate_fn=partial(collate_fn, processor=processor, device=device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08147e2a-ff55-4b30-8898-55542fbfd072",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Length of the Train Dataloader: {len(train_loader)}')\n",
    "print(f'Length of the Val Dataloader: {len(val_loader)}')\n",
    "print(f'Length of the Test Dataloader: {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd429110-01f3-40dd-be2d-347162c3f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the dataloader before the forward pass\n",
    "for batch in train_loader:\n",
    "    inputs, labels = batch\n",
    "\n",
    "    for k, v in inputs.items():\n",
    "        print(f'{k} -> {v.dtype}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08af97d-d62d-4f4f-980b-2f39df08430f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46ea2c16-69a1-4ff0-ba2d-90743717aa1f",
   "metadata": {},
   "source": [
    "### Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328c41e-562f-4e61-b3da-ef0810626c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_validation():\n",
    "    \n",
    "    qlora_qwen_model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            inputs, labels = batch\n",
    "            outputs = qlora_qwen_model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    qlora_qwen_model.train()\n",
    "    torch.cuda.empty_cache()\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2034a5-62b0-4ee3-a549-b101a85c55c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_test():\n",
    "    \n",
    "    qlora_qwen_model.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            inputs, labels = batch\n",
    "            outputs = qlora_qwen_model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    qlora_qwen_model.train()\n",
    "    torch.cuda.empty_cache()\n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3dd913-8843-4d59-9f41-3c77792ba8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80a2c2f5-63f5-4c91-b373-8f1f6e8e1c91",
   "metadata": {},
   "source": [
    "### Training Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c081744d-5fed-4acc-b578-bee8319e7ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d7c82d-a4b8-4a08-87d0-087e4db099ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 5e-5\n",
    "epochs = 1\n",
    "weight_decay = 0.00\n",
    "gradient_accumulation_steps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7926b984-8314-4547-bb03-bd80b04af485",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "best_val_loss = float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec62d9-9664-48a7-9205-4a7124084c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch     = len(train_loader) // gradient_accumulation_steps\n",
    "total_train_steps   = steps_per_epoch * epochs\n",
    "num_warmup_steps    = int(0.05 * total_train_steps)          # 5 %   (quick)  \n",
    "# ➟ for medium: 0.03 works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a30d26-b242-44c3-87fe-2fdd4cb4e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_train_steps, num_warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf509f09-5fee-48fd-bdd3-f1f2739b5e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(qlora_qwen_model.parameters(), lr=LR, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293fd2d7-25ca-4b63-9ee9-bf1a77fde21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=total_train_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db64018e-ab1b-42ae-b02e-234df2bbbbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDir = '/home/aritrad/main/Qwen2.5-VL-3B/GRPO/chkpts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6337eab2-abd9-47d1-beb4-d87058404783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6737c0f0-7809-4cd2-a0b0-6379bfcdcfa0",
   "metadata": {},
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea363c42-aeb5-4b25-9a0e-bee5677cf310",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    accumulated_loss = 0\n",
    "    \n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch\n",
    "\n",
    "        outputs = qlora_qwen_model(**inputs, labels=labels)\n",
    "        loss = outputs.loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "            \n",
    "        accumulated_loss += loss.item()\n",
    "        \n",
    "        if (idx+1) % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(qlora_qwen_model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            logger.info(f\"[ Epoch {epoch+1} | idx: {idx} | Optim Step {global_step} | Train Loss: {loss.item():.4f} ]\")\n",
    "\n",
    "            if global_step % 150 == 0:\n",
    "                avg_val_loss = do_validation()\n",
    "                logger.info(f\"Val Loss @ Optim step: {global_step} -> {avg_val_loss:.4f}\\n\")\n",
    "            \n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    qlora_qwen_model.save_pretrained(os.path.join(saveDir, 'qwen2.5-qty-chkpt'))\n",
    "                    logger.info(f\"***** ✅ Checkpoint Saved *****\\n\")\n",
    "\n",
    "    logger.info(f\"Epoch {epoch+1} completed. Avg loss: {accumulated_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56751471-64d4-4750-81c0-bdf86d0e32c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da40fdff-c442-41e0-912d-e981d5908dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
